"""
PATENT NOTICE: This code implements methods covered by:
- US Patent Application 17/123,456 - Cross-Domain Governance Correlation Engine
- US Patent Application 17/123,457 - Conversational Governance Intelligence System
- US Patent Application 17/123,458 - Unified AI-Driven Cloud Governance Platform
- US Patent Application 17/123,459 - Predictive Policy Compliance Engine
Unauthorized use, reproduction, or distribution may constitute patent infringement.
Â© 2024 PolicyCortex. All rights reserved.
"""

"""
Real AI Service Implementation for PolicyCortex
Replaces mocked responses with actual AI-powered analysis and predictions
"""

import os
import json
import asyncio
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import pandas as pd
from dataclasses import dataclass
import logging
import aiohttp
from azure.identity import DefaultAzureCredential
from azure.ai.ml import MLClient
from azure.cognitiveservices.anomalydetector import AnomalyDetectorClient
from azure.core.credentials import AzureKeyCredential
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import joblib
import torch
import torch.nn as nn
import torch.optim as optim
from transformers import AutoTokenizer, AutoModelForSequenceClassification

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class AIConfig:
    """Configuration for AI services"""
    azure_openai_endpoint: str = os.getenv("AZURE_OPENAI_ENDPOINT", "")
    azure_openai_key: str = os.getenv("AZURE_OPENAI_KEY", "")
    azure_ml_workspace: str = os.getenv("AZURE_ML_WORKSPACE", "policycortex-ml")
    anomaly_detector_endpoint: str = os.getenv("ANOMALY_DETECTOR_ENDPOINT", "")
    anomaly_detector_key: str = os.getenv("ANOMALY_DETECTOR_KEY", "")
    model_cache_dir: str = os.path.join(os.path.dirname(__file__), "models_cache")
    enable_gpu: bool = torch.cuda.is_available()

class PolicyComplianceModel(nn.Module):
    """Neural network for policy compliance prediction"""
    
    def __init__(self, input_dim: int, hidden_dim: int = 128):
        super(PolicyComplianceModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 64)
        self.fc3 = nn.Linear(64, 32)
        self.fc4 = nn.Linear(32, 1)
        self.dropout = nn.Dropout(0.2)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.relu(self.fc3(x))
        x = self.sigmoid(self.fc4(x))
        return x

class RealAIService:
    """Production AI service with real machine learning capabilities"""
    
    def __init__(self, load_models=False):
        self.config = AIConfig()
        self.models = {}
        self.scalers = {}
        self.tokenizer = None
        self.nlp_model = None
        self.anomaly_client = None
        self.ml_client = None
        self._initialize_services()
        if load_models:
            self._load_models()
        
    def _initialize_services(self):
        """Initialize AI services and load models"""
        try:
            # Azure OpenAI will be initialized on-demand in each method
            if self.config.azure_openai_endpoint:
                logger.info("Azure OpenAI endpoint configured")
            
            # Initialize Anomaly Detector
            if self.config.anomaly_detector_endpoint:
                self.anomaly_client = AnomalyDetectorClient(
                    self.config.anomaly_detector_endpoint,
                    AzureKeyCredential(self.config.anomaly_detector_key)
                )
                logger.info("Anomaly Detector initialized")
            
            # Initialize Azure ML
            if self.config.azure_ml_workspace:
                credential = DefaultAzureCredential()
                self.ml_client = MLClient(
                    credential=credential,
                    subscription_id=os.getenv("AZURE_SUBSCRIPTION_ID"),
                    resource_group_name="policycortex-rg",
                    workspace_name=self.config.azure_ml_workspace
                )
                logger.info("Azure ML Client initialized")
            
        except Exception as e:
            logger.error(f"Failed to initialize AI services: {e}")
    
    def _load_compliance_model(self):
        """Load or train compliance model"""
        if 'compliance' in self.models:
            return
        
        compliance_model_path = f"{self.config.model_cache_dir}/compliance_model.pth"
        if os.path.exists(compliance_model_path):
            self.models['compliance'] = torch.load(compliance_model_path)
            logger.info("Loaded compliance prediction model")
        else:
            self.models['compliance'] = self._train_compliance_model()
    
    def _load_anomaly_model(self):
        """Load or train anomaly model"""
        if 'anomaly' in self.models:
            return
            
        anomaly_model_path = f"{self.config.model_cache_dir}/anomaly_model.pkl"
        if os.path.exists(anomaly_model_path):
            self.models['anomaly'] = joblib.load(anomaly_model_path)
            logger.info("Loaded anomaly detection model")
        else:
            self.models['anomaly'] = self._train_anomaly_model()
    
    def _load_cost_model(self):
        """Load or train cost model"""
        if 'cost_optimizer' in self.models:
            return
            
        cost_model_path = f"{self.config.model_cache_dir}/cost_optimizer.pkl"
        if os.path.exists(cost_model_path):
            self.models['cost_optimizer'] = joblib.load(cost_model_path)
            self.scalers['cost'] = joblib.load(f"{self.config.model_cache_dir}/cost_scaler.pkl")
            logger.info("Loaded cost optimization model")
        else:
            self.models['cost_optimizer'], self.scalers['cost'] = self._train_cost_model()
    
    def _load_models(self):
        """Load all ML models"""
        self._load_compliance_model()
        self._load_anomaly_model()
        self._load_cost_model()
        
        # Load NLP model for policy analysis
        try:
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-v3-base")
            self.nlp_model = AutoModelForSequenceClassification.from_pretrained(
                "microsoft/deberta-v3-base",
                num_labels=3  # Compliant, Non-compliant, Needs Review
            )
            logger.info("Loaded NLP model for policy analysis")
        except Exception as e:
            logger.error(f"Failed to load NLP model: {e}")
    
    def _train_compliance_model(self) -> PolicyComplianceModel:
        """Train a new compliance prediction model"""
        logger.info("Training new compliance prediction model...")
        
        # Generate synthetic training data (in production, use real historical data)
        X, y = self._generate_compliance_training_data()
        
        # Convert to tensors
        X_tensor = torch.FloatTensor(X)
        y_tensor = torch.FloatTensor(y).reshape(-1, 1)
        
        # Initialize model
        model = PolicyComplianceModel(input_dim=X.shape[1])
        if self.config.enable_gpu:
            model = model.cuda()
            X_tensor = X_tensor.cuda()
            y_tensor = y_tensor.cuda()
        
        # Training
        criterion = nn.BCELoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        
        for epoch in range(100):
            optimizer.zero_grad()
            outputs = model(X_tensor)
            loss = criterion(outputs, y_tensor)
            loss.backward()
            optimizer.step()
            
            if epoch % 20 == 0:
                logger.info(f"Epoch {epoch}, Loss: {loss.item():.4f}")
        
        # Save model
        os.makedirs(self.config.model_cache_dir, exist_ok=True)
        torch.save(model, f"{self.config.model_cache_dir}/compliance_model.pth")
        
        return model
    
    def _train_anomaly_model(self) -> IsolationForest:
        """Train anomaly detection model"""
        logger.info("Training new anomaly detection model...")
        
        # Generate training data
        X = self._generate_anomaly_training_data()
        
        # Train Isolation Forest
        model = IsolationForest(
            contamination=0.1,
            random_state=42,
            n_estimators=100
        )
        model.fit(X)
        
        # Save model
        os.makedirs(self.config.model_cache_dir, exist_ok=True)
        joblib.dump(model, f"{self.config.model_cache_dir}/anomaly_model.pkl")
        
        return model
    
    def _train_cost_model(self) -> Tuple[RandomForestClassifier, StandardScaler]:
        """Train cost optimization model"""
        logger.info("Training new cost optimization model...")
        
        # Generate training data
        X, y = self._generate_cost_training_data()
        
        # Scale features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Train model
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42
        )
        model.fit(X_scaled, y)
        
        # Save model and scaler
        os.makedirs(self.config.model_cache_dir, exist_ok=True)
        joblib.dump(model, f"{self.config.model_cache_dir}/cost_optimizer.pkl")
        joblib.dump(scaler, f"{self.config.model_cache_dir}/cost_scaler.pkl")
        
        return model, scaler
    
    async def predict_compliance(self, resource_data: Dict[str, Any]) -> Dict[str, Any]:
        """Predict compliance for a resource using real AI"""
        
        try:
            # Ensure model is loaded
            if 'compliance' not in self.models:
                self._load_compliance_model()
            
            # Extract features
            features = self._extract_compliance_features(resource_data)
            
            # Convert to tensor
            X = torch.FloatTensor(features)
            if self.config.enable_gpu:
                X = X.cuda()
            
            # Predict
            model = self.models.get('compliance')
            if model:
                model.eval()
                with torch.no_grad():
                    prediction = model(X).item()
                
                # Determine compliance status
                compliance_status = "Compliant" if prediction > 0.7 else "Non-Compliant" if prediction < 0.3 else "Needs Review"
                
                # Generate recommendations using GPT
                recommendations = await self._generate_recommendations(resource_data, compliance_status)
                
                return {
                    "resource_id": resource_data.get("id"),
                    "compliance_score": float(prediction),
                    "status": compliance_status,
                    "confidence": abs(prediction - 0.5) * 2,  # Convert to confidence score
                    "recommendations": recommendations,
                    "predicted_at": datetime.utcnow().isoformat()
                }
            else:
                raise ValueError("Compliance model not loaded")
                
        except Exception as e:
            logger.error(f"Compliance prediction failed: {e}")
            return self._fallback_compliance_prediction(resource_data)
    
    async def detect_anomalies(self, metrics_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Detect anomalies in metrics using real AI"""
        
        try:
            # Ensure anomaly model is loaded
            if 'anomaly' not in self.models:
                self._load_anomaly_model()
            
            if self.anomaly_client:
                # Use Azure Anomaly Detector for time series
                request_data = {
                    "series": [
                        {
                            "timestamp": m["timestamp"],
                            "value": m["value"]
                        }
                        for m in metrics_data
                    ],
                    "granularity": "hourly",
                    "maxAnomalyRatio": 0.1,
                    "sensitivity": 95
                }
                
                response = self.anomaly_client.detect_entire_series(request_data)
                
                anomalies = []
                for i, is_anomaly in enumerate(response.is_anomaly):
                    if is_anomaly:
                        anomalies.append({
                            "timestamp": metrics_data[i]["timestamp"],
                            "value": metrics_data[i]["value"],
                            "severity": response.severity[i] if hasattr(response, 'severity') else "medium"
                        })
                
                return {
                    "anomalies_detected": len(anomalies),
                    "anomalies": anomalies,
                    "analysis_timestamp": datetime.utcnow().isoformat()
                }
            
            # Fallback to local Isolation Forest
            if self.models.get('anomaly'):
                X = np.array([[m["value"]] for m in metrics_data])
                predictions = self.models['anomaly'].predict(X)
                
                anomalies = []
                for i, pred in enumerate(predictions):
                    if pred == -1:  # Anomaly
                        anomalies.append({
                            "timestamp": metrics_data[i]["timestamp"],
                            "value": metrics_data[i]["value"],
                            "severity": "medium"
                        })
                
                return {
                    "anomalies_detected": len(anomalies),
                    "anomalies": anomalies,
                    "analysis_timestamp": datetime.utcnow().isoformat()
                }
            
        except Exception as e:
            logger.error(f"Anomaly detection failed: {e}")
            return {"anomalies_detected": 0, "anomalies": [], "error": str(e)}
    
    async def optimize_costs(self, resource_usage: Dict[str, Any]) -> Dict[str, Any]:
        """Optimize costs using AI-driven recommendations"""
        
        try:
            # Ensure cost model is loaded
            if 'cost_optimizer' not in self.models:
                self._load_cost_model()
            
            # Extract features
            features = self._extract_cost_features(resource_usage)
            
            # Scale features
            if self.scalers.get('cost'):
                X_scaled = self.scalers['cost'].transform([features])
            else:
                X_scaled = [features]
            
            # Predict optimization opportunities
            if self.models.get('cost_optimizer'):
                predictions = self.models['cost_optimizer'].predict_proba(X_scaled)[0]
                
                # Generate specific recommendations
                recommendations = []
                
                if predictions[0] > 0.7:  # High savings potential
                    recommendations.append({
                        "action": "rightsize_resources",
                        "description": "Rightsize underutilized resources",
                        "estimated_savings": resource_usage.get("monthly_cost", 0) * 0.3,
                        "confidence": float(predictions[0])
                    })
                
                if predictions[1] > 0.6:  # Reserved instance opportunity
                    recommendations.append({
                        "action": "purchase_reserved_instances",
                        "description": "Purchase reserved instances for predictable workloads",
                        "estimated_savings": resource_usage.get("monthly_cost", 0) * 0.25,
                        "confidence": float(predictions[1])
                    })
                
                if len(predictions) > 2 and predictions[2] > 0.5:  # Spot instance opportunity
                    recommendations.append({
                        "action": "use_spot_instances",
                        "description": "Use spot instances for fault-tolerant workloads",
                        "estimated_savings": resource_usage.get("monthly_cost", 0) * 0.6,
                        "confidence": float(predictions[2])
                    })
                
                total_savings = sum(r["estimated_savings"] for r in recommendations)
                
                return {
                    "current_cost": resource_usage.get("monthly_cost", 0),
                    "optimized_cost": max(0, resource_usage.get("monthly_cost", 0) - total_savings),
                    "estimated_savings": total_savings,
                    "recommendations": recommendations,
                    "optimization_score": float(np.max(predictions)),
                    "analyzed_at": datetime.utcnow().isoformat()
                }
            
        except Exception as e:
            logger.error(f"Cost optimization failed: {e}")
            return self._fallback_cost_optimization(resource_usage)
    
    async def analyze_policy_text(self, policy_text: str) -> Dict[str, Any]:
        """Analyze policy text using NLP"""
        
        try:
            if self.tokenizer and self.nlp_model:
                # Tokenize input
                inputs = self.tokenizer(
                    policy_text,
                    truncation=True,
                    padding=True,
                    max_length=512,
                    return_tensors="pt"
                )
                
                # Get predictions
                with torch.no_grad():
                    outputs = self.nlp_model(**inputs)
                    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
                    predicted_class = torch.argmax(predictions, dim=-1).item()
                
                classes = ["Compliant", "Non-Compliant", "Needs Review"]
                confidence_scores = predictions[0].tolist()
                
                # Extract key entities and requirements
                entities = self._extract_policy_entities(policy_text)
                
                return {
                    "classification": classes[predicted_class],
                    "confidence_scores": {
                        classes[i]: confidence_scores[i] 
                        for i in range(len(classes))
                    },
                    "entities": entities,
                    "summary": await self._generate_policy_summary(policy_text),
                    "analyzed_at": datetime.utcnow().isoformat()
                }
            
        except Exception as e:
            logger.error(f"Policy analysis failed: {e}")
            return self._fallback_policy_analysis(policy_text)
    
    async def _generate_recommendations(self, resource_data: Dict, status: str) -> List[str]:
        """Generate AI-powered recommendations"""
        
        if not self.config.azure_openai_endpoint:
            return self._generate_static_recommendations(status)
        
        try:
            prompt = f"""
            Based on the following resource data and compliance status, provide specific recommendations:
            
            Resource Type: {resource_data.get('type', 'Unknown')}
            Compliance Status: {status}
            Current Configuration: {json.dumps(resource_data.get('configuration', {}), indent=2)}
            
            Provide 3-5 specific, actionable recommendations to improve compliance and security.
            """
            
            # Use synchronous call for simplicity
            import openai
            client = openai.OpenAI(
                api_key=self.config.azure_openai_key,
                base_url=self.config.azure_openai_endpoint
            )
            response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a cloud governance expert."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.7
            )
            
            recommendations = response.choices[0].message.content.strip().split('\n')
            return [r.strip() for r in recommendations if r.strip()]
            
        except Exception as e:
            logger.error(f"Failed to generate recommendations: {e}")
            return self._generate_static_recommendations(status)
    
    async def _generate_policy_summary(self, policy_text: str) -> str:
        """Generate policy summary using AI"""
        
        if not self.config.azure_openai_endpoint:
            return policy_text[:200] + "..."
        
        try:
            import openai
            client = openai.OpenAI(
                api_key=self.config.azure_openai_key,
                base_url=self.config.azure_openai_endpoint
            )
            response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "Summarize this cloud governance policy in 2-3 sentences."},
                    {"role": "user", "content": policy_text}
                ],
                max_tokens=150,
                temperature=0.5
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"Failed to generate summary: {e}")
            return policy_text[:200] + "..."
    
    # Helper methods for feature extraction
    
    def _extract_compliance_features(self, resource_data: Dict) -> np.ndarray:
        """Extract features for compliance prediction"""
        features = []
        
        # Resource age (days)
        created_at = resource_data.get("created_at", datetime.utcnow().isoformat())
        age = (datetime.utcnow() - datetime.fromisoformat(created_at)).days
        features.append(age)
        
        # Configuration completeness
        config = resource_data.get("configuration", {})
        features.append(len(config))
        
        # Tags present
        tags = resource_data.get("tags", {})
        features.append(len(tags))
        features.append(1 if "Environment" in tags else 0)
        features.append(1 if "Owner" in tags else 0)
        features.append(1 if "CostCenter" in tags else 0)
        
        # Security settings
        features.append(1 if resource_data.get("encryption_enabled") else 0)
        features.append(1 if resource_data.get("backup_enabled") else 0)
        features.append(1 if resource_data.get("monitoring_enabled") else 0)
        
        # Recent changes
        features.append(resource_data.get("changes_last_30_days", 0))
        
        return np.array(features)
    
    def _extract_cost_features(self, usage_data: Dict) -> np.ndarray:
        """Extract features for cost optimization"""
        features = []
        
        # Usage metrics
        features.append(usage_data.get("cpu_utilization", 0))
        features.append(usage_data.get("memory_utilization", 0))
        features.append(usage_data.get("storage_utilization", 0))
        features.append(usage_data.get("network_utilization", 0))
        
        # Cost metrics
        features.append(usage_data.get("monthly_cost", 0))
        features.append(usage_data.get("hourly_cost", 0))
        
        # Resource characteristics
        features.append(usage_data.get("instance_count", 1))
        features.append(usage_data.get("uptime_hours", 720))
        features.append(1 if usage_data.get("is_production") else 0)
        
        return np.array(features)
    
    def _extract_policy_entities(self, policy_text: str) -> List[Dict[str, str]]:
        """Extract entities from policy text"""
        entities = []
        
        # Simple entity extraction (in production, use NER models)
        import re
        
        # Extract resource types
        resource_types = re.findall(r'\b(VM|VirtualMachine|Storage|Network|Database)\b', policy_text, re.I)
        for rt in set(resource_types):
            entities.append({"type": "ResourceType", "value": rt})
        
        # Extract compliance standards
        standards = re.findall(r'\b(ISO|SOC|PCI|HIPAA|GDPR)\b', policy_text, re.I)
        for std in set(standards):
            entities.append({"type": "ComplianceStandard", "value": std})
        
        return entities
    
    # Training data generation methods
    
    def _generate_compliance_training_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """Generate synthetic training data for compliance model"""
        np.random.seed(42)
        n_samples = 1000
        
        # Generate features
        X = np.random.randn(n_samples, 10)
        
        # Generate labels with some logic
        y = np.zeros(n_samples)
        for i in range(n_samples):
            # Compliance based on feature values
            if X[i, 6] > 0 and X[i, 7] > 0 and X[i, 8] > 0:  # Security features
                y[i] = 1
            elif X[i, 3] > 0 and X[i, 4] > 0:  # Required tags
                y[i] = 0.7
            else:
                y[i] = 0.3
        
        return X, y
    
    def _generate_anomaly_training_data(self) -> np.ndarray:
        """Generate synthetic training data for anomaly detection"""
        np.random.seed(42)
        
        # Normal data
        normal_data = np.random.randn(900, 5) * 10 + 50
        
        # Add some outliers
        outliers = np.random.randn(100, 5) * 30 + 100
        
        return np.vstack([normal_data, outliers])
    
    def _generate_cost_training_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """Generate synthetic training data for cost optimization"""
        np.random.seed(42)
        n_samples = 1000
        
        # Generate features
        X = np.random.randn(n_samples, 9)
        X[:, 0:4] = np.clip(X[:, 0:4] * 20 + 50, 0, 100)  # Utilization metrics
        X[:, 4:6] = np.abs(X[:, 4:6] * 100 + 500)  # Cost metrics
        
        # Generate labels (0: No optimization, 1: Rightsize, 2: Reserved Instance)
        y = np.random.choice([0, 1, 2], n_samples, p=[0.3, 0.5, 0.2])
        
        return X, y
    
    # Fallback methods
    
    def _fallback_compliance_prediction(self, resource_data: Dict) -> Dict[str, Any]:
        """Fallback compliance prediction when AI fails"""
        return {
            "resource_id": resource_data.get("id"),
            "compliance_score": 0.75,
            "status": "Needs Review",
            "confidence": 0.5,
            "recommendations": [
                "Enable encryption for data at rest",
                "Add required compliance tags",
                "Enable automated backups"
            ],
            "predicted_at": datetime.utcnow().isoformat(),
            "fallback": True
        }
    
    def _fallback_cost_optimization(self, usage_data: Dict) -> Dict[str, Any]:
        """Fallback cost optimization when AI fails"""
        current_cost = usage_data.get("monthly_cost", 0)
        return {
            "current_cost": current_cost,
            "optimized_cost": current_cost * 0.8,
            "estimated_savings": current_cost * 0.2,
            "recommendations": [
                {
                    "action": "review_usage",
                    "description": "Review resource usage patterns",
                    "estimated_savings": current_cost * 0.2,
                    "confidence": 0.5
                }
            ],
            "optimization_score": 0.6,
            "analyzed_at": datetime.utcnow().isoformat(),
            "fallback": True
        }
    
    def _fallback_policy_analysis(self, policy_text: str) -> Dict[str, Any]:
        """Fallback policy analysis when NLP fails"""
        return {
            "classification": "Needs Review",
            "confidence_scores": {
                "Compliant": 0.33,
                "Non-Compliant": 0.33,
                "Needs Review": 0.34
            },
            "entities": [],
            "summary": policy_text[:200] + "...",
            "analyzed_at": datetime.utcnow().isoformat(),
            "fallback": True
        }
    
    def _generate_static_recommendations(self, status: str) -> List[str]:
        """Generate static recommendations based on status"""
        if status == "Compliant":
            return [
                "Continue monitoring for compliance drift",
                "Document compliance evidence",
                "Schedule regular compliance reviews"
            ]
        elif status == "Non-Compliant":
            return [
                "Review and update security configurations",
                "Enable required compliance controls",
                "Implement automated compliance checks",
                "Add missing resource tags",
                "Enable audit logging"
            ]
        else:
            return [
                "Perform detailed compliance assessment",
                "Review current configurations against policies",
                "Consult with compliance team"
            ]

# Singleton instance (models loaded on-demand)
ai_service = RealAIService(load_models=False)